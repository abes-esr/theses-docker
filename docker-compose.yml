# Configuration docker pour déployer theses.fr
# avant de lancer docker-compose up, il est nécessaire
# de copier le fichier .env-dist dans le fichier .env
# puis de personnaliser le contenu du fichier .env
version: "3.5"

services:

  # theses-rp est le reverse proxy application de theses.fr
  # toutes les requêtes HTTP passent par lui, il est chargé
  # d'authentifier les utilisateurs avec la fédération d'identités
  # RENATER lorsque ce derniers accèdent à /74979_GERARDIN_2018_archivage.pdf
  # (à adapter)
  theses-rp:
    container_name: theses-rp
    image: abesesr/docker-shibboleth-renater-sp:1.3.0
    ports:
      - ${THESES_RP_HTTPS_PORT}:443
      - ${THESES_RP_HTTP_PORT}:80
    environment:
      RENATER_SP_TEST_OR_PROD: ${RENATER_SP_TEST_OR_PROD}
      RENATER_SP_ENTITY_ID: ${RENATER_SP_ENTITY_ID}
      RENATER_SP_ADMIN_MAIL: ${RENATER_SP_ADMIN_MAIL}
      RENATER_SP_HTTPD_SERVER_NAME: ${RENATER_SP_HTTPD_SERVER_NAME}
      RENATER_SP_HTTPD_LOG_LEVEL: "info ssl:warn"
      RENATER_SP_HTTPD_LOG_FORMAT: '%h \"%{Shib-Identity-Provider}i\" \"%{eppn}i\" \"%{primary-affiliation}i\" \"%{supannEtablissement}i\" %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"'
      RENATER_SP_HTTPD_PUBLIC_PATH_0: "/"
      RENATER_SP_HTTPD_PUBLIC_PROXY_TO_0: "http://theses-front:80/"
      RENATER_SP_HTTPD_PUBLIC_PATH_1: "/poc-fede/"
      RENATER_SP_HTTPD_PUBLIC_PROXY_TO_1: "http://theses-api-diffusion:80/poc-fede/"
      RENATER_SP_HTTPD_PUBLIC_PATH_2: "/api/v1/recherche/"
      RENATER_SP_HTTPD_PUBLIC_PROXY_TO_2: "http://theses-api-recherche:80/api/v1/recherche/"
      RENATER_SP_HTTPD_PUBLIC_PATH_3: "/kibana/"
      RENATER_SP_HTTPD_PUBLIC_PROXY_TO_3: "http://theses-kibana:5601/"
      RENATER_SP_HTTPD_PROTECTED_PATH_0: "/poc-fede/74979_GERARDIN_2018_archivage.pdf"
      RENATER_SP_HTTPD_PROTECTED_PROXY_TO_0: "http://theses-api-diffusion/poc-fede/74979_GERARDIN_2018_archivage.pdf"
    volumes:
      - ./volumes/theses-rp/shibboleth/ssl/:/etc/shibboleth/ssl/
    restart: unless-stopped
    # Ces commentaires sont conservés pour expliquer pourquoi theses-rp ne dépend pas des autres conteneurs
    # la raison est que theses-rp est relativement long à démarrer (plusieurs minutes) et qu'il ne sera que
    # très rarrement mis à jour, donc autant ne pas le redémarrer si jamais une de ses dépendance redémarre.
    #depends_on:
    #  - theses-api-diffusion
    #  - theses-api-recherche
    #  - theses-front
    labels:
      # pour envoyer les logs dans le puits de log de l'abes
      - "co.elastic.logs/enabled=true"
      - "co.elastic.logs/processors.add_fields.target="
      - "co.elastic.logs/processors.add_fields.fields.abes_appli=theses"
      - "co.elastic.logs/processors.add_fields.fields.abes_middleware=apache"



  # Interface utilisateur de theses.fr (dévelopée en VueJS)
  theses-front:
    container_name: theses-front
    image: abesesr/theses:${THESES_FRONT_VERSION}
    restart: unless-stopped
    ports:
      - ${THESES_FRONT_HTTP_PORT}:80
    depends_on:
      - theses-api-diffusion
      - theses-api-recherche
    labels:
      # pour envoyer les logs dans le puits de log de l'abes
      - "co.elastic.logs/enabled=true"
      - "co.elastic.logs/processors.add_fields.target="
      - "co.elastic.logs/processors.add_fields.fields.abes_appli=theses"
      - "co.elastic.logs/processors.add_fields.fields.abes_middleware=nginx"



  # API de diffusion des thèses en java spring de theses.fr
  # (travail en cours, pour la preuve de concept il est
  #  chargé de mettre à disposition un PDF en 
  #  accès restreint que theses-rp protège)
  theses-api-diffusion:
    container_name: theses-api-diffusion
    #image: mendhak/http-https-echo:23
    build: ./images/theses-api-diffusion/
    image: httpd:2.4.54-theses-api-diffusion
    restart: unless-stopped
    labels:
      # pour envoyer les logs dans le puits de log de l'abes
      - "co.elastic.logs/enabled=true"
      - "co.elastic.logs/processors.add_fields.target="
      - "co.elastic.logs/processors.add_fields.fields.abes_appli=theses"
      - "co.elastic.logs/processors.add_fields.fields.abes_middleware=apache"



  # API de recherche des thèses qui fait l'intermédiaire avec elasticsearch
  theses-api-recherche:
    container_name: theses-api-recherche
    # TODO : à remplacer par sa version en java spring
    build: ./images/theses-api-recherche/
    image: abesesr/theses:develop-api-recherche
    restart: unless-stopped
    environment:
      THESES_API_RECHERCHE_ELASTIC_USERNAME: ${THESES_API_RECHERCHE_ELASTIC_USERNAME}
      THESES_API_RECHERCHE_ELASTIC_PASSWORD: ${THESES_API_RECHERCHE_ELASTIC_PASSWORD}
      ELASTICSEARCH_HOST_PROXY_TO: "https://theses-elasticsearch-01:9200/"
    depends_on:
      theses-elasticsearch:
        condition: service_healthy
    labels:
      # pour envoyer les logs dans le puits de log de l'abes
      - "co.elastic.logs/enabled=true"
      - "co.elastic.logs/processors.add_fields.target="
      - "co.elastic.logs/processors.add_fields.fields.abes_appli=theses"
      - "co.elastic.logs/processors.add_fields.fields.abes_middleware=apache"




  # batch pour indexer les 11 thèses dans elasticsearch
  # (travail en cours pour la preuve de concept de l'indexation de 11 thèses)
  theses-batch-11theses:
    container_name: theses-batch-11theses
    depends_on:
      theses-elasticsearch:
        condition: service_healthy
    build: ./images/theses-batch/
    image: abesesr/theses:develop-batch
    environment:
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      ELASTICSEARCH_HOST: "https://theses-elasticsearch-01:9200"
    command: /app/theses-sample-load.sh
    labels:
      # pour envoyer les logs dans le puits de log de l'abes
      - "co.elastic.logs/enabled=true"
      - "co.elastic.logs/processors.add_fields.target="
      - "co.elastic.logs/processors.add_fields.fields.abes_appli=theses"
      - "co.elastic.logs/processors.add_fields.fields.abes_middleware=adhoc"



  #######################################
  # theses-elasticsearch et theses-kibana
  # toute la configuration pour lancer elasticsearch et kibana de theses.fr
  # Ref. https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html
  #
  # theses-elasticsearch-setupcerts
  # conteneur chargé de configurer les certificats pour que les 
  # 3 noeuds elasticsearch communiquent entre eux de façon sécurisé
  theses-elasticsearch-setupcerts:
    container_name: theses-elasticsearch-setupcerts
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELK_STACK_VERSION}
    volumes:
      - ./volumes/theses-elasticsearch-setupcerts/:/usr/share/elasticsearch/config/certs/
    user: "0"
    environment:
      KIBANA_PASSWORD: ${KIBANA_PASSWORD}
      ELASTIC_PASSWORD: ${ELASTIC_PASSWORD}
      THESES_API_RECHERCHE_ELASTIC_USERNAME: ${THESES_API_RECHERCHE_ELASTIC_USERNAME} 
      THESES_API_RECHERCHE_ELASTIC_PASSWORD: ${THESES_API_RECHERCHE_ELASTIC_PASSWORD} 
    command: >
      bash -c '
        if [ x${ELASTIC_PASSWORD} == x ]; then
          echo "Set the ELASTIC_PASSWORD environment variable in the .env file";
          exit 1;
        elif [ x${KIBANA_PASSWORD} == x ]; then
          echo "Set the KIBANA_PASSWORD environment variable in the .env file";
          exit 1;
        fi;
        if [ ! -f config/certs/ca.zip ]; then
          echo "Creating CA";
          bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;
          unzip config/certs/ca.zip -d config/certs;
        fi;
        if [ ! -f config/certs/certs.zip ]; then
          echo "Creating certs";
          echo -ne \
          "instances:\n"\
          "  - name: es01\n"\
          "    dns:\n"\
          "      - theses-elasticsearch-01\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          "  - name: es02\n"\
          "    dns:\n"\
          "      - theses-elasticsearch-02\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          "  - name: es03\n"\
          "    dns:\n"\
          "      - theses-elasticsearch-03\n"\
          "      - localhost\n"\
          "    ip:\n"\
          "      - 127.0.0.1\n"\
          > config/certs/instances.yml;
          bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;
          unzip config/certs/certs.zip -d config/certs;
        fi;
        echo "Setting file permissions"
        chown -R root:root config/certs;
        find . -type d -exec chmod 750 \{\} \;;
        find . -type f -exec chmod 640 \{\} \;;
        echo "Waiting for Elasticsearch availability";
        until curl -s --cacert config/certs/ca/ca.crt https://theses-elasticsearch-01:9200 | grep -q "missing authentication credentials"; do sleep 30; done;
        echo "Setting kibana_system password";
        until curl -s -X POST --cacert config/certs/ca/ca.crt -u elastic:${ELASTIC_PASSWORD} -H "Content-Type: application/json" https://theses-elasticsearch-01:9200/_security/user/kibana_system/_password -d "{\"password\":\"${KIBANA_PASSWORD}\"}" | grep -q "^{}"; do sleep 10; done;
        echo "Setting theses-api-recherche password";
        until curl -s -X POST --cacert config/certs/ca/ca.crt -u elastic:${ELASTIC_PASSWORD} -H "Content-Type: application/json" https://theses-elasticsearch-01:9200/_security/user/${THESES_API_RECHERCHE_ELASTIC_USERNAME} -d "{\"password\":\"${THESES_API_RECHERCHE_ELASTIC_PASSWORD}\", \"enabled\": true, \"roles\":[\"viewer\"], \"full_name\":\"\", \"email\":\"\"}" | grep -q "^{\"created\":"; do sleep 10; done;
        echo "All done!";
      '
    healthcheck:
      test: ["CMD-SHELL", "[ -f config/certs/es01/es01.crt ]"]
      interval: 1s
      timeout: 5s
      retries: 120
    labels:
      # pour envoyer les logs dans le puits de log de l'abes
      - "co.elastic.logs/enabled=true"
      - "co.elastic.logs/processors.add_fields.target="
      - "co.elastic.logs/processors.add_fields.fields.abes_appli=theses"
      - "co.elastic.logs/processors.add_fields.fields.abes_middleware=adhoc"



  # theses-elasticsearch-01
  # noeud numéro1 d'elasticsearch (en local c'est l'unique noeud qui est lancé)
  theses-elasticsearch:
    container_name: theses-elasticsearch-${ELK_CLUSTER_NODE_NUMBER}
    depends_on:
      theses-elasticsearch-setupcerts:
        condition: service_healthy
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELK_STACK_VERSION}
    volumes:
      - ./volumes/theses-elasticsearch-setupcerts/:/usr/share/elasticsearch/config/certs/
      - ./volumes/theses-elasticsearch/:/usr/share/elasticsearch/data/
    ports:
      - ${ELK_ELASTIC_HTTP_PORT}:9200
      - ${ELK_ELASTIC_TRANSPORT_PORT}:9300
    environment:
      - node.name=theses-es${ELK_CLUSTER_NODE_NUMBER}
      - cluster.name=${ELK_CLUSTER_NAME}
      - cluster.initial_master_nodes=${ELK_CLUSTER_INITIAL_MASTER_NODES}
      - discovery.seed_hosts=${ELK_CLUSTER_DISCOVER_SEED_HOSTS}
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es${ELK_CLUSTER_NODE_NUMBER}/es${ELK_CLUSTER_NODE_NUMBER}.key
      - xpack.security.http.ssl.certificate=certs/es${ELK_CLUSTER_NODE_NUMBER}/es${ELK_CLUSTER_NODE_NUMBER}.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.http.ssl.verification_mode=certificate
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es${ELK_CLUSTER_NODE_NUMBER}/es${ELK_CLUSTER_NODE_NUMBER}.key
      - xpack.security.transport.ssl.certificate=certs/es${ELK_CLUSTER_NODE_NUMBER}/es${ELK_CLUSTER_NODE_NUMBER}.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${ELK_LICENSE}
    mem_limit: ${ELK_MEM_LIMIT}
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    labels:
      # pour envoyer les logs dans le puits de log de l'abes
      - "co.elastic.logs/enabled=true"
      - "co.elastic.logs/processors.add_fields.target="
      - "co.elastic.logs/processors.add_fields.fields.abes_appli=theses"
      - "co.elastic.logs/processors.add_fields.fields.abes_middleware=elasticsearch"



  # theses-kibana
  # kibana est le backoffice de l'elasticsearch de theses.fr
  theses-kibana:
    container_name: theses-kibana
    depends_on:
      theses-elasticsearch:
        condition: service_healthy
    image: docker.elastic.co/kibana/kibana:${ELK_STACK_VERSION}
    volumes:
      - ./volumes/theses-elasticsearch-setupcerts/:/usr/share/kibana/config/certs/
      - ./volumes/theses-kibana/:/usr/share/kibana/data/
    ports:
      - ${ELK_KIBANA_PORT}:5601
    environment:
      - SERVERNAME=kibana
      - SERVER_BASEPATH=/kibana
      - ELASTICSEARCH_HOSTS=https://theses-elasticsearch-01:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
      - xpack.reporting.roles.enabled=false
    mem_limit: ${ELK_MEM_LIMIT}
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    labels:
      # pour envoyer les logs dans le puits de log de l'abes
      - "co.elastic.logs/enabled=true"
      - "co.elastic.logs/processors.add_fields.target="
      - "co.elastic.logs/processors.add_fields.fields.abes_appli=theses"
      - "co.elastic.logs/processors.add_fields.fields.abes_middleware=kibana"



# Cette configuration peut être décommentée pour pouvoir monter un cluster local elasticsearch
# elle permet de lancer plusieurs noeuds indépendants au niveau docker mais reliés par ce réseau commun
# pour lancer deux noeuds sur la même machine, il suffit alors de :
# 1) créer ce réseau via "docker network create theses-docker-es-cluster-network"
# 2) pour le noeud 2 utiliser la config suivante : https://github.com/abes-esr/theses-es-cluster-docker/
# 3) paramétrer le .env de chaque noeud avec ELK_CLUSTER_NODE_NUMBER qui vaut "01" ou "02" (fonction du noeud) :
#    ELK_CLUSTER_NODE_NUMBER=01
#    ELK_CLUSTER_DISCOVER_SEED_HOSTS=theses-elasticsearch-01:9300,theses-elasticsearch-02:9300
#    ELK_CLUSTER_INITIAL_MASTER_NODES=theses-es01,theses-es02
# 4) lancer les deux noeuds dans leurs deux répertoires dédié avec "docker-compose up"
#networks:
#  default:
#    name: theses-docker-es-cluster-network
#    external: true

